{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2tX8qfpUUcA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NyYiXsBuUVBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t_Bg22qAUVEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torch transformers accelerate bitsandbytes langchain sentence-transformers faiss-cpu openpyxl pacmap datasets langchain-community ragatouille pypdf\n"
      ],
      "metadata": {
        "id": "RsrYA3a_UVG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HuggingFaceH4/zephyr-7b-beta"
      ],
      "metadata": {
        "id": "LSV6O5m9W9Fa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Ø§Ù„ØªØ«Ø¨ÙŠØªØ§Øª\n",
        "#!pip install -q torch transformers accelerate bitsandbytes langchain sentence-transformers faiss-cpu openpyxl pacmap datasets langchain-community ragatouille pypdf\n",
        "\n",
        "# 2. Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª\n",
        "from tqdm.notebook import tqdm\n",
        "import pandas as pd\n",
        "from typing import Optional, List, Tuple\n",
        "import matplotlib.pyplot as plt\n",
        "from langchain.docstore.document import Document as LangchainDocument\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores.utils import DistanceStrategy\n",
        "from ragatouille import RAGPretrainedModel\n",
        "from pypdf import PdfReader\n",
        "import torch\n",
        "# 3. Ø§Ù„Ø«ÙˆØ§Ø¨Øª\n",
        "EMBEDDING_MODEL_NAME = \"thenlper/gte-small\"\n",
        "READER_MODEL_NAME = \"HuggingFaceH4/zephyr-7b-beta\"\n",
        "PDF_FILEPATH = \"/content/The_Lightning_Thief_-_Percy_Jackson_1-10.pdf\"  # ğŸ‘ˆ Ø§Ø³ØªØ¨Ø¯Ù„ Ù‡Ø°Ø§ Ø¨Ù…Ø³Ø§Ø± Ù…Ù„Ù PDF Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ\n",
        "CHUNK_SIZE = 512\n",
        "CHUNK_OVERLAP = int(CHUNK_SIZE / 10)\n",
        "\n",
        "# 4. Ø¯ÙˆØ§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© PDF\n",
        "\n",
        "def load_pdf(filepath: str) -> List[LangchainDocument]:\n",
        "    \"\"\"\n",
        "    ÙŠØ­Ù…Ù„ Ù…Ù„Ù PDF ÙˆÙŠØ­ÙˆÙ„Ù‡ Ø¥Ù„Ù‰ Ù‚Ø§Ø¦Ù…Ø© Ù…Ù† LangchainDocument.\n",
        "    \"\"\"\n",
        "    reader = PdfReader(filepath)\n",
        "    documents = []\n",
        "    for page_num, page in enumerate(tqdm(reader.pages, desc=\"Processing pages\")):\n",
        "        text = page.extract_text()\n",
        "        metadata = {\"source\": filepath, \"page\": page_num + 1}\n",
        "        documents.append(LangchainDocument(page_content=text, metadata=metadata))\n",
        "    return documents\n",
        "\n",
        "def split_documents(\n",
        "    chunk_size: int,\n",
        "    knowledge_base: List[LangchainDocument],\n",
        "    tokenizer_name: Optional[str] = EMBEDDING_MODEL_NAME,\n",
        ") -> List[LangchainDocument]:\n",
        "    \"\"\"\n",
        "    ÙŠÙ‚Ø³Ù… Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø¥Ù„Ù‰ Ø£Ø¬Ø²Ø§Ø¡ (chunks) Ø¨Ø­Ø¬Ù… Ù…Ø­Ø¯Ø¯.\n",
        "    \"\"\"\n",
        "    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
        "        AutoTokenizer.from_pretrained(tokenizer_name),\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=CHUNK_OVERLAP,\n",
        "        add_start_index=True,\n",
        "        strip_whitespace=True,\n",
        "        separators=[\"\\\\n\\\\n\", \"\\\\n\", \" \", \"\"], # ÙÙˆØ§ØµÙ„ Ù…Ù†Ø§Ø³Ø¨Ø© Ù„Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ\n",
        "    )\n",
        "\n",
        "    docs_processed = []\n",
        "    for doc in knowledge_base:\n",
        "        docs_processed += text_splitter.split_documents([doc])\n",
        "\n",
        "    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª\n",
        "    unique_texts = {}\n",
        "    docs_processed_unique = []\n",
        "    for doc in docs_processed:\n",
        "        if doc.page_content not in unique_texts:\n",
        "            unique_texts[doc.page_content] = True\n",
        "            docs_processed_unique.append(doc)\n",
        "\n",
        "    return docs_processed_unique\n",
        "\n",
        "# 5. Ø¯ÙˆØ§Ù„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬\n",
        "\n",
        "def setup_embedding_model():\n",
        "    \"\"\"\n",
        "    ÙŠÙ‡ÙŠØ¦ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¶Ù…ÙŠÙ† (embedding model).\n",
        "    \"\"\"\n",
        "    embedding_model = HuggingFaceEmbeddings(\n",
        "        model_name=EMBEDDING_MODEL_NAME,\n",
        "        multi_process=True,\n",
        "        model_kwargs={\"device\": \"cuda\"},\n",
        "        encode_kwargs={\"normalize_embeddings\": True},  # Ù…Ù‡Ù… Ù„Ù€ cosine similarity\n",
        "    )\n",
        "    return embedding_model\n",
        "\n",
        "def setup_reader_model():\n",
        "    \"\"\"\n",
        "    ÙŠÙ‡ÙŠØ¦ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù‚Ø§Ø±Ø¦ (reader model).\n",
        "    \"\"\"\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    )\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        READER_MODEL_NAME, quantization_config=bnb_config\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(READER_MODEL_NAME)\n",
        "\n",
        "    reader_llm = pipeline(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        task=\"text-generation\",\n",
        "        do_sample=True,\n",
        "        temperature=0.2,\n",
        "        repetition_penalty=1.1,\n",
        "        return_full_text=False,\n",
        "        max_new_tokens=500,\n",
        "    )\n",
        "    return reader_llm\n",
        "\n",
        "def setup_reranker():\n",
        "    \"\"\"\n",
        "    ÙŠÙ‡ÙŠØ¦ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ±ØªÙŠØ¨ (reranker).\n",
        "    \"\"\"\n",
        "    reranker = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
        "    return reranker\n",
        "\n",
        "# 6. Ø¯Ø§Ù„Ø© Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©\n",
        "\n",
        "def create_knowledge_base(\n",
        "    documents: List[LangchainDocument], embedding_model\n",
        ") -> FAISS:\n",
        "    \"\"\"\n",
        "    ÙŠÙ†Ø´Ø¦ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ù…ØªØ¬Ù‡Ø© (vector database).\n",
        "    \"\"\"\n",
        "    knowledge_base = FAISS.from_documents(\n",
        "        documents, embedding_model, distance_strategy=DistanceStrategy.COSINE\n",
        "    )\n",
        "    return knowledge_base\n",
        "\n",
        "# 7. Ø¯Ø§Ù„Ø© Ø±ÙˆØ¨ÙˆØª Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©\n",
        "\n",
        "def answer_with_rag(\n",
        "    question: str,\n",
        "    llm: pipeline,\n",
        "    knowledge_index: FAISS,\n",
        "    reranker: Optional[RAGPretrainedModel] = None,\n",
        "    num_retrieved_docs: int = 30,\n",
        "    num_docs_final: int = 5,\n",
        ") -> Tuple[str, List[str]]:\n",
        "    \"\"\"\n",
        "    ÙŠØ¬ÙŠØ¨ Ø¹Ù„Ù‰ Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… RAG.\n",
        "    \"\"\"\n",
        "    # 1. Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª\n",
        "    print(\"=> Retrieving documents...\")\n",
        "    relevant_docs = knowledge_index.similarity_search(\n",
        "        query=question, k=num_retrieved_docs\n",
        "    )\n",
        "    relevant_docs_content = [doc.page_content for doc in relevant_docs]\n",
        "\n",
        "    # 2. Ø¥Ø¹Ø§Ø¯Ø© ØªØ±ØªÙŠØ¨ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)\n",
        "    if reranker:\n",
        "        print(\"=> Reranking documents...\")\n",
        "        relevant_docs_content = reranker.rerank(question, relevant_docs_content, k=num_docs_final)\n",
        "        relevant_docs_content = [doc[\"content\"] for doc in relevant_docs_content]\n",
        "\n",
        "    relevant_docs_content = relevant_docs_content[:num_docs_final]\n",
        "\n",
        "    # 3. ØµÙŠØ§ØºØ© Ø§Ù„ prompt\n",
        "    context = \"\\\\nExtracted documents:\\\\n\"\n",
        "    context += \"\".join(\n",
        "        [f\"Document {str(i)}:::\\\\n{doc}\" for i, doc in enumerate(relevant_docs_content)]\n",
        "    )\n",
        "\n",
        "    prompt_in_chat_format = [\n",
        "      {\n",
        "          \"role\": \"system\",\n",
        "          \"content\": \"\"\"Using the information contained in the context,\n",
        "    give a comprehensive answer to the question.\n",
        "    Respond only to the question asked, response should be concise and relevant to the question.\n",
        "    Provide the number of the source document when relevant.\n",
        "    If the answer cannot be deduced from the context, do not give an answer.\"\"\"\n",
        "      },\n",
        "      {\n",
        "          \"role\": \"user\",\n",
        "          \"content\": f\"\"\"Context:\n",
        "    {context}\n",
        "    ---\n",
        "    Now here is the question you need to answer.\n",
        "\n",
        "    Question: {question}\"\"\"\n",
        "      },\n",
        "    ]\n",
        "    tokenizer = AutoTokenizer.from_pretrained(READER_MODEL_NAME)\n",
        "    final_prompt = tokenizer.apply_chat_template(\n",
        "      prompt_in_chat_format, tokenize=False, add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    # 4. ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©\n",
        "    print(\"=> Generating answer...\")\n",
        "    answer = llm(final_prompt)[0][\"generated_text\"]\n",
        "\n",
        "    return answer, relevant_docs_content\n",
        "\n",
        "# 8. Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„ØªØ´ØºÙŠÙ„ Ø±ÙˆØ¨ÙˆØª Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©.\n",
        "    \"\"\"\n",
        "    # 1. ØªØ­Ù…ÙŠÙ„ ÙˆÙ…Ø¹Ø§Ù„Ø¬Ø© PDF\n",
        "    raw_knowledge_base = load_pdf(PDF_FILEPATH)\n",
        "    processed_docs = split_documents(CHUNK_SIZE, raw_knowledge_base)\n",
        "\n",
        "    # 2. ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬\n",
        "    embedding_model = setup_embedding_model()\n",
        "    reader_llm = setup_reader_model()\n",
        "    reranker = setup_reranker()\n",
        "\n",
        "    # 3. Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©\n",
        "    knowledge_base = create_knowledge_base(processed_docs, embedding_model)\n",
        "\n",
        "    # 4. Ø·Ø±Ø­ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©\n",
        "    question = \"How many pages in the book?\" # ğŸ‘ˆ  Ø§Ø·Ø±Ø­ Ø³Ø¤Ø§Ù„Ùƒ Ù‡Ù†Ø§\n",
        "    answer, relevant_docs = answer_with_rag(\n",
        "        question, reader_llm, knowledge_base, reranker=reranker\n",
        "    )\n",
        "\n",
        "    # 5. Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬\n",
        "    print(\"=\" * 50 + \"Answer\" + \"=\" * 50)\n",
        "    print(f\"{answer}\")\n",
        "    print(\"=\" * 50 + \"Source docs\" + \"=\" * 50)\n",
        "    for i, doc in enumerate(relevant_docs):\n",
        "        print(f\"Document {i} (Page: {processed_docs[i].metadata['page']}) {'-' * 40}\")\n",
        "        print(doc)\n",
        "\n",
        "# 9. ØªØ´ØºÙŠÙ„ Ø§Ù„ÙƒÙˆØ¯\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "yZkNe3NXUVJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O9qaFNH-W08q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mbEsi3aUW056"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EQLlZ4sPW026"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### meta-llama/Llama-3.2-3B-Instruct"
      ],
      "metadata": {
        "id": "JSA_JzCeW5WW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Ø§Ù„ØªØ«Ø¨ÙŠØªØ§Øª\n",
        "#!pip install -q torch transformers accelerate bitsandbytes langchain sentence-transformers faiss-cpu openpyxl pacmap datasets langchain-community ragatouille pypdf\n",
        "\n",
        "# 2. Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª\n",
        "from tqdm.notebook import tqdm\n",
        "import pandas as pd\n",
        "from typing import Optional, List, Tuple\n",
        "import matplotlib.pyplot as plt\n",
        "from langchain.docstore.document import Document as LangchainDocument\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores.utils import DistanceStrategy\n",
        "from ragatouille import RAGPretrainedModel\n",
        "from pypdf import PdfReader\n",
        "import torch\n",
        "# 3. Ø§Ù„Ø«ÙˆØ§Ø¨Øª\n",
        "EMBEDDING_MODEL_NAME = \"thenlper/gte-small\"\n",
        "READER_MODEL_NAME = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "PDF_FILEPATH = \"/content/The_Lightning_Thief_-_Percy_Jackson_1-10.pdf\"  # ğŸ‘ˆ Ø§Ø³ØªØ¨Ø¯Ù„ Ù‡Ø°Ø§ Ø¨Ù…Ø³Ø§Ø± Ù…Ù„Ù PDF Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ\n",
        "CHUNK_SIZE = 512\n",
        "CHUNK_OVERLAP = int(CHUNK_SIZE / 10)\n",
        "\n",
        "# 4. Ø¯ÙˆØ§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© PDF\n",
        "\n",
        "def load_pdf(filepath: str) -> List[LangchainDocument]:\n",
        "    \"\"\"\n",
        "    Loads a PDF file and converts it to a list from LangchainDocument.\n",
        "    \"\"\"\n",
        "    reader = PdfReader(filepath)\n",
        "    documents = []\n",
        "    for page_num, page in enumerate(tqdm(reader.pages, desc=\"Processing pages\")):\n",
        "        text = page.extract_text()\n",
        "        metadata = {\"source\": filepath, \"page\": page_num + 1}\n",
        "        documents.append(LangchainDocument(page_content=text, metadata=metadata))\n",
        "    return documents\n",
        "\n",
        "def split_documents(\n",
        "    chunk_size: int,\n",
        "    knowledge_base: List[LangchainDocument],\n",
        "    tokenizer_name: Optional[str] = EMBEDDING_MODEL_NAME,\n",
        ") -> List[LangchainDocument]:\n",
        "    \"\"\"\n",
        "    Splits documents into chunks of a specified size.\n",
        "    \"\"\"\n",
        "    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
        "        AutoTokenizer.from_pretrained(tokenizer_name),\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=CHUNK_OVERLAP,\n",
        "        add_start_index=True,\n",
        "        strip_whitespace=True,\n",
        "        separators=[\"\\\\n\\\\n\", \"\\\\n\", \" \", \"\"], # ÙÙˆØ§ØµÙ„ Ù…Ù†Ø§Ø³Ø¨Ø© Ù„Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ\n",
        "    )\n",
        "\n",
        "    docs_processed = []\n",
        "    for doc in knowledge_base:\n",
        "        docs_processed += text_splitter.split_documents([doc])\n",
        "\n",
        "    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª\n",
        "    unique_texts = {}\n",
        "    docs_processed_unique = []\n",
        "    for doc in docs_processed:\n",
        "        if doc.page_content not in unique_texts:\n",
        "            unique_texts[doc.page_content] = True\n",
        "            docs_processed_unique.append(doc)\n",
        "\n",
        "    return docs_processed_unique\n",
        "\n",
        "# 5. Ø¯ÙˆØ§Ù„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬\n",
        "\n",
        "def setup_embedding_model():\n",
        "    \"\"\"\n",
        "    ÙŠÙ‡ÙŠØ¦ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¶Ù…ÙŠÙ† (embedding model).\n",
        "    \"\"\"\n",
        "    embedding_model = HuggingFaceEmbeddings(\n",
        "        model_name=EMBEDDING_MODEL_NAME,\n",
        "        multi_process=True,\n",
        "        model_kwargs={\"device\": \"cuda\"},\n",
        "        encode_kwargs={\"normalize_embeddings\": True},  # Ù…Ù‡Ù… Ù„Ù€ cosine similarity\n",
        "    )\n",
        "    return embedding_model\n",
        "\n",
        "def setup_reader_model():\n",
        "    \"\"\"\n",
        "    ÙŠÙ‡ÙŠØ¦ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù‚Ø§Ø±Ø¦ (reader model).\n",
        "    \"\"\"\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    )\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        READER_MODEL_NAME, quantization_config=bnb_config\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(READER_MODEL_NAME)\n",
        "\n",
        "    reader_llm = pipeline(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        task=\"text-generation\",\n",
        "        do_sample=True,\n",
        "        temperature=0.2,\n",
        "        repetition_penalty=1.1,\n",
        "        return_full_text=False,\n",
        "        max_new_tokens=500,\n",
        "    )\n",
        "    return reader_llm\n",
        "\n",
        "def setup_reranker():\n",
        "    \"\"\"\n",
        "    Prepares the reranker model.\n",
        "    \"\"\"\n",
        "    reranker = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
        "    return reranker\n",
        "\n",
        "# 6. Ø¯Ø§Ù„Ø© Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©\n",
        "\n",
        "def create_knowledge_base(\n",
        "    documents: List[LangchainDocument], embedding_model\n",
        ") -> FAISS:\n",
        "    \"\"\"\n",
        "    Creates a vector database.\n",
        "    \"\"\"\n",
        "    knowledge_base = FAISS.from_documents(\n",
        "        documents, embedding_model, distance_strategy=DistanceStrategy.COSINE\n",
        "    )\n",
        "    return knowledge_base\n",
        "\n",
        "# 7. Ø¯Ø§Ù„Ø© Ø±ÙˆØ¨ÙˆØª Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©\n",
        "\n",
        "def answer_with_rag(\n",
        "    question: str,\n",
        "    llm: pipeline,\n",
        "    knowledge_index: FAISS,\n",
        "    reranker: Optional[RAGPretrainedModel] = None,\n",
        "    num_retrieved_docs: int = 30,\n",
        "    num_docs_final: int = 5,\n",
        ") -> Tuple[str, List[str]]:\n",
        "    \"\"\"\n",
        "    Answers the user's question using RAG.\n",
        "    \"\"\"\n",
        "    # 1. Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª\n",
        "    print(\"=> Retrieving documents...\")\n",
        "    relevant_docs = knowledge_index.similarity_search(\n",
        "        query=question, k=num_retrieved_docs\n",
        "    )\n",
        "    relevant_docs_content = [doc.page_content for doc in relevant_docs]\n",
        "\n",
        "    # 2. Ø¥Ø¹Ø§Ø¯Ø© ØªØ±ØªÙŠØ¨ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)\n",
        "    if reranker:\n",
        "        print(\"=> Reranking documents...\")\n",
        "        relevant_docs_content = reranker.rerank(question, relevant_docs_content, k=num_docs_final)\n",
        "        relevant_docs_content = [doc[\"content\"] for doc in relevant_docs_content]\n",
        "\n",
        "    relevant_docs_content = relevant_docs_content[:num_docs_final]\n",
        "\n",
        "    # 3. ØµÙŠØ§ØºØ© Ø§Ù„ prompt\n",
        "    context = \"\\\\nExtracted documents:\\\\n\"\n",
        "    context += \"\".join(\n",
        "        [f\"Document {str(i)}:::\\\\n{doc}\" for i, doc in enumerate(relevant_docs_content)]\n",
        "    )\n",
        "\n",
        "    prompt_in_chat_format = [\n",
        "      {\n",
        "          \"role\": \"system\",\n",
        "          \"content\": \"\"\"Using the information contained in the context,\n",
        "    give a comprehensive answer to the question.\n",
        "    Respond only to the question asked, response should be concise and relevant to the question.\n",
        "    Provide the number of the source document when relevant.\n",
        "    If the answer cannot be deduced from the context, do not give an answer.\"\"\"\n",
        "      },\n",
        "      {\n",
        "          \"role\": \"user\",\n",
        "          \"content\": f\"\"\"Context:\n",
        "    {context}\n",
        "    ---\n",
        "    Now here is the question you need to answer.\n",
        "\n",
        "    Question: {question}\"\"\"\n",
        "      },\n",
        "    ]\n",
        "    tokenizer = AutoTokenizer.from_pretrained(READER_MODEL_NAME)\n",
        "    final_prompt = tokenizer.apply_chat_template(\n",
        "      prompt_in_chat_format, tokenize=False, add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    # 4. ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©\n",
        "    print(\"=> Generating answer...\")\n",
        "    answer = llm(final_prompt)[0][\"generated_text\"]\n",
        "\n",
        "    return answer, relevant_docs_content\n",
        "\n",
        "# 8. Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    The main function to run the chatbot.\n",
        "    \"\"\"\n",
        "    # 1. ØªØ­Ù…ÙŠÙ„ ÙˆÙ…Ø¹Ø§Ù„Ø¬Ø© PDF\n",
        "    raw_knowledge_base = load_pdf(PDF_FILEPATH)\n",
        "    processed_docs = split_documents(CHUNK_SIZE, raw_knowledge_base)\n",
        "\n",
        "    # 2. ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬\n",
        "    embedding_model = setup_embedding_model()\n",
        "    reader_llm = setup_reader_model()\n",
        "    reranker = setup_reranker()\n",
        "\n",
        "    # 3. Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©\n",
        "    knowledge_base = create_knowledge_base(processed_docs, embedding_model)\n",
        "\n",
        "    # 4. Ø·Ø±Ø­ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©\n",
        "    question = \"How many pages in the book?\" # ğŸ‘ˆ  Ø§Ø·Ø±Ø­ Ø³Ø¤Ø§Ù„Ùƒ Ù‡Ù†Ø§\n",
        "    answer, relevant_docs = answer_with_rag(\n",
        "        question, reader_llm, knowledge_base, reranker=reranker\n",
        "    )\n",
        "\n",
        "    # 5. Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬\n",
        "    print(\"=\" * 50 + \"Answer\" + \"=\" * 50)\n",
        "    print(f\"{answer}\")\n",
        "    print(\"=\" * 50 + \"Source docs\" + \"=\" * 50)\n",
        "    for i, doc in enumerate(relevant_docs):\n",
        "        print(f\"Document {i} (Page: {processed_docs[i].metadata['page']}) {'-' * 40}\")\n",
        "        print(doc)\n",
        "\n",
        "# 9. ØªØ´ØºÙŠÙ„ Ø§Ù„ÙƒÙˆØ¯\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "vF_c8r65W0zz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}