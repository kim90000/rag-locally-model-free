{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2tX8qfpUUcA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NyYiXsBuUVBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t_Bg22qAUVEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torch transformers accelerate bitsandbytes langchain sentence-transformers faiss-cpu openpyxl pacmap datasets langchain-community ragatouille pypdf\n"
      ],
      "metadata": {
        "id": "RsrYA3a_UVG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HuggingFaceH4/zephyr-7b-beta"
      ],
      "metadata": {
        "id": "LSV6O5m9W9Fa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. التثبيتات\n",
        "#!pip install -q torch transformers accelerate bitsandbytes langchain sentence-transformers faiss-cpu openpyxl pacmap datasets langchain-community ragatouille pypdf\n",
        "\n",
        "# 2. المكتبات\n",
        "from tqdm.notebook import tqdm\n",
        "import pandas as pd\n",
        "from typing import Optional, List, Tuple\n",
        "import matplotlib.pyplot as plt\n",
        "from langchain.docstore.document import Document as LangchainDocument\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores.utils import DistanceStrategy\n",
        "from ragatouille import RAGPretrainedModel\n",
        "from pypdf import PdfReader\n",
        "import torch\n",
        "# 3. الثوابت\n",
        "EMBEDDING_MODEL_NAME = \"thenlper/gte-small\"\n",
        "READER_MODEL_NAME = \"HuggingFaceH4/zephyr-7b-beta\"\n",
        "PDF_FILEPATH = \"/content/The_Lightning_Thief_-_Percy_Jackson_1-10.pdf\"  # 👈 استبدل هذا بمسار ملف PDF الخاص بك\n",
        "CHUNK_SIZE = 512\n",
        "CHUNK_OVERLAP = int(CHUNK_SIZE / 10)\n",
        "\n",
        "# 4. دوال معالجة PDF\n",
        "\n",
        "def load_pdf(filepath: str) -> List[LangchainDocument]:\n",
        "    \"\"\"\n",
        "    يحمل ملف PDF ويحوله إلى قائمة من LangchainDocument.\n",
        "    \"\"\"\n",
        "    reader = PdfReader(filepath)\n",
        "    documents = []\n",
        "    for page_num, page in enumerate(tqdm(reader.pages, desc=\"Processing pages\")):\n",
        "        text = page.extract_text()\n",
        "        metadata = {\"source\": filepath, \"page\": page_num + 1}\n",
        "        documents.append(LangchainDocument(page_content=text, metadata=metadata))\n",
        "    return documents\n",
        "\n",
        "def split_documents(\n",
        "    chunk_size: int,\n",
        "    knowledge_base: List[LangchainDocument],\n",
        "    tokenizer_name: Optional[str] = EMBEDDING_MODEL_NAME,\n",
        ") -> List[LangchainDocument]:\n",
        "    \"\"\"\n",
        "    يقسم المستندات إلى أجزاء (chunks) بحجم محدد.\n",
        "    \"\"\"\n",
        "    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
        "        AutoTokenizer.from_pretrained(tokenizer_name),\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=CHUNK_OVERLAP,\n",
        "        add_start_index=True,\n",
        "        strip_whitespace=True,\n",
        "        separators=[\"\\\\n\\\\n\", \"\\\\n\", \" \", \"\"], # فواصل مناسبة للنص العربي\n",
        "    )\n",
        "\n",
        "    docs_processed = []\n",
        "    for doc in knowledge_base:\n",
        "        docs_processed += text_splitter.split_documents([doc])\n",
        "\n",
        "    # إزالة التكرارات\n",
        "    unique_texts = {}\n",
        "    docs_processed_unique = []\n",
        "    for doc in docs_processed:\n",
        "        if doc.page_content not in unique_texts:\n",
        "            unique_texts[doc.page_content] = True\n",
        "            docs_processed_unique.append(doc)\n",
        "\n",
        "    return docs_processed_unique\n",
        "\n",
        "# 5. دوال النماذج\n",
        "\n",
        "def setup_embedding_model():\n",
        "    \"\"\"\n",
        "    يهيئ نموذج التضمين (embedding model).\n",
        "    \"\"\"\n",
        "    embedding_model = HuggingFaceEmbeddings(\n",
        "        model_name=EMBEDDING_MODEL_NAME,\n",
        "        multi_process=True,\n",
        "        model_kwargs={\"device\": \"cuda\"},\n",
        "        encode_kwargs={\"normalize_embeddings\": True},  # مهم لـ cosine similarity\n",
        "    )\n",
        "    return embedding_model\n",
        "\n",
        "def setup_reader_model():\n",
        "    \"\"\"\n",
        "    يهيئ نموذج القارئ (reader model).\n",
        "    \"\"\"\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    )\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        READER_MODEL_NAME, quantization_config=bnb_config\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(READER_MODEL_NAME)\n",
        "\n",
        "    reader_llm = pipeline(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        task=\"text-generation\",\n",
        "        do_sample=True,\n",
        "        temperature=0.2,\n",
        "        repetition_penalty=1.1,\n",
        "        return_full_text=False,\n",
        "        max_new_tokens=500,\n",
        "    )\n",
        "    return reader_llm\n",
        "\n",
        "def setup_reranker():\n",
        "    \"\"\"\n",
        "    يهيئ نموذج إعادة الترتيب (reranker).\n",
        "    \"\"\"\n",
        "    reranker = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
        "    return reranker\n",
        "\n",
        "# 6. دالة إنشاء قاعدة المعرفة\n",
        "\n",
        "def create_knowledge_base(\n",
        "    documents: List[LangchainDocument], embedding_model\n",
        ") -> FAISS:\n",
        "    \"\"\"\n",
        "    ينشئ قاعدة المعرفة المتجهة (vector database).\n",
        "    \"\"\"\n",
        "    knowledge_base = FAISS.from_documents(\n",
        "        documents, embedding_model, distance_strategy=DistanceStrategy.COSINE\n",
        "    )\n",
        "    return knowledge_base\n",
        "\n",
        "# 7. دالة روبوت المحادثة\n",
        "\n",
        "def answer_with_rag(\n",
        "    question: str,\n",
        "    llm: pipeline,\n",
        "    knowledge_index: FAISS,\n",
        "    reranker: Optional[RAGPretrainedModel] = None,\n",
        "    num_retrieved_docs: int = 30,\n",
        "    num_docs_final: int = 5,\n",
        ") -> Tuple[str, List[str]]:\n",
        "    \"\"\"\n",
        "    يجيب على سؤال المستخدم باستخدام RAG.\n",
        "    \"\"\"\n",
        "    # 1. استرجاع المستندات\n",
        "    print(\"=> Retrieving documents...\")\n",
        "    relevant_docs = knowledge_index.similarity_search(\n",
        "        query=question, k=num_retrieved_docs\n",
        "    )\n",
        "    relevant_docs_content = [doc.page_content for doc in relevant_docs]\n",
        "\n",
        "    # 2. إعادة ترتيب المستندات (اختياري)\n",
        "    if reranker:\n",
        "        print(\"=> Reranking documents...\")\n",
        "        relevant_docs_content = reranker.rerank(question, relevant_docs_content, k=num_docs_final)\n",
        "        relevant_docs_content = [doc[\"content\"] for doc in relevant_docs_content]\n",
        "\n",
        "    relevant_docs_content = relevant_docs_content[:num_docs_final]\n",
        "\n",
        "    # 3. صياغة ال prompt\n",
        "    context = \"\\\\nExtracted documents:\\\\n\"\n",
        "    context += \"\".join(\n",
        "        [f\"Document {str(i)}:::\\\\n{doc}\" for i, doc in enumerate(relevant_docs_content)]\n",
        "    )\n",
        "\n",
        "    prompt_in_chat_format = [\n",
        "      {\n",
        "          \"role\": \"system\",\n",
        "          \"content\": \"\"\"Using the information contained in the context,\n",
        "    give a comprehensive answer to the question.\n",
        "    Respond only to the question asked, response should be concise and relevant to the question.\n",
        "    Provide the number of the source document when relevant.\n",
        "    If the answer cannot be deduced from the context, do not give an answer.\"\"\"\n",
        "      },\n",
        "      {\n",
        "          \"role\": \"user\",\n",
        "          \"content\": f\"\"\"Context:\n",
        "    {context}\n",
        "    ---\n",
        "    Now here is the question you need to answer.\n",
        "\n",
        "    Question: {question}\"\"\"\n",
        "      },\n",
        "    ]\n",
        "    tokenizer = AutoTokenizer.from_pretrained(READER_MODEL_NAME)\n",
        "    final_prompt = tokenizer.apply_chat_template(\n",
        "      prompt_in_chat_format, tokenize=False, add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    # 4. توليد الإجابة\n",
        "    print(\"=> Generating answer...\")\n",
        "    answer = llm(final_prompt)[0][\"generated_text\"]\n",
        "\n",
        "    return answer, relevant_docs_content\n",
        "\n",
        "# 8. الدالة الرئيسية\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    الدالة الرئيسية لتشغيل روبوت المحادثة.\n",
        "    \"\"\"\n",
        "    # 1. تحميل ومعالجة PDF\n",
        "    raw_knowledge_base = load_pdf(PDF_FILEPATH)\n",
        "    processed_docs = split_documents(CHUNK_SIZE, raw_knowledge_base)\n",
        "\n",
        "    # 2. تهيئة النماذج\n",
        "    embedding_model = setup_embedding_model()\n",
        "    reader_llm = setup_reader_model()\n",
        "    reranker = setup_reranker()\n",
        "\n",
        "    # 3. إنشاء قاعدة المعرفة\n",
        "    knowledge_base = create_knowledge_base(processed_docs, embedding_model)\n",
        "\n",
        "    # 4. طرح الأسئلة\n",
        "    question = \"How many pages in the book?\" # 👈  اطرح سؤالك هنا\n",
        "    answer, relevant_docs = answer_with_rag(\n",
        "        question, reader_llm, knowledge_base, reranker=reranker\n",
        "    )\n",
        "\n",
        "    # 5. طباعة النتائج\n",
        "    print(\"=\" * 50 + \"Answer\" + \"=\" * 50)\n",
        "    print(f\"{answer}\")\n",
        "    print(\"=\" * 50 + \"Source docs\" + \"=\" * 50)\n",
        "    for i, doc in enumerate(relevant_docs):\n",
        "        print(f\"Document {i} (Page: {processed_docs[i].metadata['page']}) {'-' * 40}\")\n",
        "        print(doc)\n",
        "\n",
        "# 9. تشغيل الكود\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "yZkNe3NXUVJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O9qaFNH-W08q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mbEsi3aUW056"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EQLlZ4sPW026"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### meta-llama/Llama-3.2-3B-Instruct"
      ],
      "metadata": {
        "id": "JSA_JzCeW5WW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. التثبيتات\n",
        "#!pip install -q torch transformers accelerate bitsandbytes langchain sentence-transformers faiss-cpu openpyxl pacmap datasets langchain-community ragatouille pypdf\n",
        "\n",
        "# 2. المكتبات\n",
        "from tqdm.notebook import tqdm\n",
        "import pandas as pd\n",
        "from typing import Optional, List, Tuple\n",
        "import matplotlib.pyplot as plt\n",
        "from langchain.docstore.document import Document as LangchainDocument\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores.utils import DistanceStrategy\n",
        "from ragatouille import RAGPretrainedModel\n",
        "from pypdf import PdfReader\n",
        "import torch\n",
        "# 3. الثوابت\n",
        "EMBEDDING_MODEL_NAME = \"thenlper/gte-small\"\n",
        "READER_MODEL_NAME = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "PDF_FILEPATH = \"/content/The_Lightning_Thief_-_Percy_Jackson_1-10.pdf\"  # 👈 استبدل هذا بمسار ملف PDF الخاص بك\n",
        "CHUNK_SIZE = 512\n",
        "CHUNK_OVERLAP = int(CHUNK_SIZE / 10)\n",
        "\n",
        "# 4. دوال معالجة PDF\n",
        "\n",
        "def load_pdf(filepath: str) -> List[LangchainDocument]:\n",
        "    \"\"\"\n",
        "    Loads a PDF file and converts it to a list from LangchainDocument.\n",
        "    \"\"\"\n",
        "    reader = PdfReader(filepath)\n",
        "    documents = []\n",
        "    for page_num, page in enumerate(tqdm(reader.pages, desc=\"Processing pages\")):\n",
        "        text = page.extract_text()\n",
        "        metadata = {\"source\": filepath, \"page\": page_num + 1}\n",
        "        documents.append(LangchainDocument(page_content=text, metadata=metadata))\n",
        "    return documents\n",
        "\n",
        "def split_documents(\n",
        "    chunk_size: int,\n",
        "    knowledge_base: List[LangchainDocument],\n",
        "    tokenizer_name: Optional[str] = EMBEDDING_MODEL_NAME,\n",
        ") -> List[LangchainDocument]:\n",
        "    \"\"\"\n",
        "    Splits documents into chunks of a specified size.\n",
        "    \"\"\"\n",
        "    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
        "        AutoTokenizer.from_pretrained(tokenizer_name),\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=CHUNK_OVERLAP,\n",
        "        add_start_index=True,\n",
        "        strip_whitespace=True,\n",
        "        separators=[\"\\\\n\\\\n\", \"\\\\n\", \" \", \"\"], # فواصل مناسبة للنص العربي\n",
        "    )\n",
        "\n",
        "    docs_processed = []\n",
        "    for doc in knowledge_base:\n",
        "        docs_processed += text_splitter.split_documents([doc])\n",
        "\n",
        "    # إزالة التكرارات\n",
        "    unique_texts = {}\n",
        "    docs_processed_unique = []\n",
        "    for doc in docs_processed:\n",
        "        if doc.page_content not in unique_texts:\n",
        "            unique_texts[doc.page_content] = True\n",
        "            docs_processed_unique.append(doc)\n",
        "\n",
        "    return docs_processed_unique\n",
        "\n",
        "# 5. دوال النماذج\n",
        "\n",
        "def setup_embedding_model():\n",
        "    \"\"\"\n",
        "    يهيئ نموذج التضمين (embedding model).\n",
        "    \"\"\"\n",
        "    embedding_model = HuggingFaceEmbeddings(\n",
        "        model_name=EMBEDDING_MODEL_NAME,\n",
        "        multi_process=True,\n",
        "        model_kwargs={\"device\": \"cuda\"},\n",
        "        encode_kwargs={\"normalize_embeddings\": True},  # مهم لـ cosine similarity\n",
        "    )\n",
        "    return embedding_model\n",
        "\n",
        "def setup_reader_model():\n",
        "    \"\"\"\n",
        "    يهيئ نموذج القارئ (reader model).\n",
        "    \"\"\"\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    )\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        READER_MODEL_NAME, quantization_config=bnb_config\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(READER_MODEL_NAME)\n",
        "\n",
        "    reader_llm = pipeline(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        task=\"text-generation\",\n",
        "        do_sample=True,\n",
        "        temperature=0.2,\n",
        "        repetition_penalty=1.1,\n",
        "        return_full_text=False,\n",
        "        max_new_tokens=500,\n",
        "    )\n",
        "    return reader_llm\n",
        "\n",
        "def setup_reranker():\n",
        "    \"\"\"\n",
        "    Prepares the reranker model.\n",
        "    \"\"\"\n",
        "    reranker = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
        "    return reranker\n",
        "\n",
        "# 6. دالة إنشاء قاعدة المعرفة\n",
        "\n",
        "def create_knowledge_base(\n",
        "    documents: List[LangchainDocument], embedding_model\n",
        ") -> FAISS:\n",
        "    \"\"\"\n",
        "    Creates a vector database.\n",
        "    \"\"\"\n",
        "    knowledge_base = FAISS.from_documents(\n",
        "        documents, embedding_model, distance_strategy=DistanceStrategy.COSINE\n",
        "    )\n",
        "    return knowledge_base\n",
        "\n",
        "# 7. دالة روبوت المحادثة\n",
        "\n",
        "def answer_with_rag(\n",
        "    question: str,\n",
        "    llm: pipeline,\n",
        "    knowledge_index: FAISS,\n",
        "    reranker: Optional[RAGPretrainedModel] = None,\n",
        "    num_retrieved_docs: int = 30,\n",
        "    num_docs_final: int = 5,\n",
        ") -> Tuple[str, List[str]]:\n",
        "    \"\"\"\n",
        "    Answers the user's question using RAG.\n",
        "    \"\"\"\n",
        "    # 1. استرجاع المستندات\n",
        "    print(\"=> Retrieving documents...\")\n",
        "    relevant_docs = knowledge_index.similarity_search(\n",
        "        query=question, k=num_retrieved_docs\n",
        "    )\n",
        "    relevant_docs_content = [doc.page_content for doc in relevant_docs]\n",
        "\n",
        "    # 2. إعادة ترتيب المستندات (اختياري)\n",
        "    if reranker:\n",
        "        print(\"=> Reranking documents...\")\n",
        "        relevant_docs_content = reranker.rerank(question, relevant_docs_content, k=num_docs_final)\n",
        "        relevant_docs_content = [doc[\"content\"] for doc in relevant_docs_content]\n",
        "\n",
        "    relevant_docs_content = relevant_docs_content[:num_docs_final]\n",
        "\n",
        "    # 3. صياغة ال prompt\n",
        "    context = \"\\\\nExtracted documents:\\\\n\"\n",
        "    context += \"\".join(\n",
        "        [f\"Document {str(i)}:::\\\\n{doc}\" for i, doc in enumerate(relevant_docs_content)]\n",
        "    )\n",
        "\n",
        "    prompt_in_chat_format = [\n",
        "      {\n",
        "          \"role\": \"system\",\n",
        "          \"content\": \"\"\"Using the information contained in the context,\n",
        "    give a comprehensive answer to the question.\n",
        "    Respond only to the question asked, response should be concise and relevant to the question.\n",
        "    Provide the number of the source document when relevant.\n",
        "    If the answer cannot be deduced from the context, do not give an answer.\"\"\"\n",
        "      },\n",
        "      {\n",
        "          \"role\": \"user\",\n",
        "          \"content\": f\"\"\"Context:\n",
        "    {context}\n",
        "    ---\n",
        "    Now here is the question you need to answer.\n",
        "\n",
        "    Question: {question}\"\"\"\n",
        "      },\n",
        "    ]\n",
        "    tokenizer = AutoTokenizer.from_pretrained(READER_MODEL_NAME)\n",
        "    final_prompt = tokenizer.apply_chat_template(\n",
        "      prompt_in_chat_format, tokenize=False, add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    # 4. توليد الإجابة\n",
        "    print(\"=> Generating answer...\")\n",
        "    answer = llm(final_prompt)[0][\"generated_text\"]\n",
        "\n",
        "    return answer, relevant_docs_content\n",
        "\n",
        "# 8. الدالة الرئيسية\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    The main function to run the chatbot.\n",
        "    \"\"\"\n",
        "    # 1. تحميل ومعالجة PDF\n",
        "    raw_knowledge_base = load_pdf(PDF_FILEPATH)\n",
        "    processed_docs = split_documents(CHUNK_SIZE, raw_knowledge_base)\n",
        "\n",
        "    # 2. تهيئة النماذج\n",
        "    embedding_model = setup_embedding_model()\n",
        "    reader_llm = setup_reader_model()\n",
        "    reranker = setup_reranker()\n",
        "\n",
        "    # 3. إنشاء قاعدة المعرفة\n",
        "    knowledge_base = create_knowledge_base(processed_docs, embedding_model)\n",
        "\n",
        "    # 4. طرح الأسئلة\n",
        "    question = \"How many pages in the book?\" # 👈  اطرح سؤالك هنا\n",
        "    answer, relevant_docs = answer_with_rag(\n",
        "        question, reader_llm, knowledge_base, reranker=reranker\n",
        "    )\n",
        "\n",
        "    # 5. طباعة النتائج\n",
        "    print(\"=\" * 50 + \"Answer\" + \"=\" * 50)\n",
        "    print(f\"{answer}\")\n",
        "    print(\"=\" * 50 + \"Source docs\" + \"=\" * 50)\n",
        "    for i, doc in enumerate(relevant_docs):\n",
        "        print(f\"Document {i} (Page: {processed_docs[i].metadata['page']}) {'-' * 40}\")\n",
        "        print(doc)\n",
        "\n",
        "# 9. تشغيل الكود\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "vF_c8r65W0zz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}